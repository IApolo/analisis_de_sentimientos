{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento Avanzado y Modelado con BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Objetivo :**\n",
    "- Configurar el entorno para utilizar modelos avanzados como BERT.\n",
    "- Preparar los datos procesados para ser ingresados al modelo.\n",
    "- Entrenar un modelo de análisis de sentimientos basado en BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Celda 1: Configuración inicial**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Verificar si se dispone de GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Ruta al dataset procesado\n",
    "data_path = r\"../outputs/cleaned_data.csv\"\n",
    "\n",
    "# Cargar el dataset procesado\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Inspeccionar las primeras filas del dataset\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(df.head())\n",
    "print(\"\\nDistribución de sentimientos:\")\n",
    "print(df[\"sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Celda 2: Preparar el tokenizer de BERT**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizando los textos...\n",
      "Tokenización completa.\n",
      "Forma de input_ids: torch.Size([9252, 128])\n",
      "Forma de attention_masks: torch.Size([9252, 128])\n"
     ]
    }
   ],
   "source": [
    "# Cargar el tokenizer preentrenado de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Función para tokenizar los textos\n",
    "def tokenize_texts(texts, tokenizer, max_len=128):\n",
    "    input_ids, attention_masks = [], []\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids.append(encoded[\"input_ids\"])\n",
    "        attention_masks.append(encoded[\"attention_mask\"])\n",
    "\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Tokenizar los textos del dataset\n",
    "print(\"Tokenizando los textos...\")\n",
    "input_ids, attention_masks = tokenize_texts(df[\"processed_text\"].values, tokenizer)\n",
    "\n",
    "# Mapear las etiquetas de sentimiento a valores numéricos\n",
    "label_mapping = {\"positivo\": 0, \"neutral\": 1, \"negativo\": 2}\n",
    "df[\"label\"] = df[\"sentiment\"].map(label_mapping)\n",
    "labels = torch.tensor(df[\"label\"].values)\n",
    "\n",
    "print(\"Tokenización completa.\")\n",
    "print(f\"Forma de input_ids: {input_ids.shape}\")\n",
    "print(f\"Forma de attention_masks: {attention_masks.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Celda 3: Dividir los datos en entrenamiento y validación**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos divididos y DataLoaders creados.\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en entrenamiento y validación\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Crear DataLoaders\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)\n",
    "\n",
    "print(\"Datos divididos y DataLoaders creados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Celda 4: Configurar el modelo BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado y configurado.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo preentrenado con una capa de clasificación\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=3,  # Tres clases: positivo, neutral, negativo\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Mover el modelo al dispositivo seleccionado\n",
    "model.to(device)\n",
    "print(\"Modelo cargado y configurado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Celda 5: Configurar optimizador y bucle de entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CondaEnvs\\ia_env\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Configurar optimizador\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "\n",
    "# Configurar scheduler para ajustar la tasa de aprendizaje\n",
    "define_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Número de épocas\n",
    "epochs = 3\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch_input_ids, batch_masks, batch_labels = [item.to(device) for item in batch]\n",
    "\n",
    "        # Reiniciar gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_masks, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Pérdida promedio por época: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Celda 6: Evaluación del modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluación en el conjunto de validación\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        batch_input_ids, batch_masks, batch_labels = [item.to(device) for item in batch]\n",
    "\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_masks)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        labels = batch_labels.cpu().numpy()\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "        # Mostrar reporte de clasificación\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"positivo\", \"neutral\", \"negativo\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Notas finales:**\n",
    "1. Este notebook entrena un modelo BERT para análisis de sentimientos con datos tokenizados y procesados previamente.\n",
    "2. Configuramos tanto el entrenamiento como la evaluación con métricas estándar.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
